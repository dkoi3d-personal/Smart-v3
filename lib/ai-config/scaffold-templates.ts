/**
 * AI Service Scaffold Templates
 *
 * These templates are injected into built applications when AI features are requested.
 * They provide a production-ready AI integration layer with:
 * - Multi-provider support (OpenAI, Anthropic, Ollama)
 * - Automatic fallback to local LLMs
 * - Streaming support
 * - Error handling with retries
 */

import { AIConfig } from './types';

export interface ScaffoldFile {
  path: string;
  content: string;
}

/**
 * Generate the AI config module
 */
export function generateAIConfigFile(config: AIConfig): ScaffoldFile {
  return {
    path: 'lib/ai/config.ts',
    content: `// AI Provider Configuration
// Generated by AI Fleet Orchestrator

export type AIProvider = 'openai' | 'anthropic' | 'ollama' | 'groq';

export interface AIProviderConfig {
  provider: AIProvider;
  apiKey?: string;
  baseUrl?: string;
  model: string;
}

export function getAIConfig(): AIProviderConfig {
  const provider = (process.env.AI_PROVIDER || '${config.defaultProvider}') as AIProvider;

  switch (provider) {
    case 'openai':
      if (!process.env.OPENAI_API_KEY) {
        throw new Error('OPENAI_API_KEY environment variable is required');
      }
      return {
        provider: 'openai',
        apiKey: process.env.OPENAI_API_KEY,
        model: process.env.OPENAI_MODEL || '${config.providers.openai.defaultModel || 'gpt-4o-mini'}',
      };

    case 'anthropic':
      if (!process.env.ANTHROPIC_API_KEY) {
        throw new Error('ANTHROPIC_API_KEY environment variable is required');
      }
      return {
        provider: 'anthropic',
        apiKey: process.env.ANTHROPIC_API_KEY,
        model: process.env.ANTHROPIC_MODEL || '${config.providers.anthropic.defaultModel || 'claude-3-haiku-20240307'}',
      };

    case 'ollama':
      return {
        provider: 'ollama',
        baseUrl: process.env.OLLAMA_BASE_URL || '${config.localLLM.ollama.baseUrl}',
        model: process.env.OLLAMA_MODEL || '${config.localLLM.ollama.defaultModel}',
      };

    case 'groq':
      if (!process.env.GROQ_API_KEY) {
        throw new Error('GROQ_API_KEY environment variable is required');
      }
      return {
        provider: 'groq',
        apiKey: process.env.GROQ_API_KEY,
        model: process.env.GROQ_MODEL || '${config.providers.groq.defaultModel || 'llama-3.1-70b-versatile'}',
      };

    default:
      throw new Error(\`Unknown AI provider: \${provider}\`);
  }
}

export function getFallbackConfig(): AIProviderConfig | null {
  // Try Ollama as fallback if configured
  if (process.env.OLLAMA_BASE_URL || process.env.ENABLE_LOCAL_FALLBACK === 'true') {
    return {
      provider: 'ollama',
      baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
      model: process.env.OLLAMA_MODEL || 'llama3.2',
    };
  }
  return null;
}
`,
  };
}

/**
 * Generate the AI service module
 */
export function generateAIServiceFile(): ScaffoldFile {
  return {
    path: 'lib/ai/service.ts',
    content: `// AI Service - Multi-provider support with fallback
// Generated by AI Fleet Orchestrator

import { getAIConfig, getFallbackConfig, AIProviderConfig } from './config';

export interface GenerateOptions {
  model?: string;
  maxTokens?: number;
  temperature?: number;
  systemPrompt?: string;
}

interface AIProvider {
  generate(prompt: string, options?: GenerateOptions): Promise<string>;
  generateStream?(prompt: string, options?: GenerateOptions): AsyncGenerator<string>;
}

// OpenAI Provider
class OpenAIProvider implements AIProvider {
  constructor(private apiKey: string) {}

  async generate(prompt: string, options: GenerateOptions = {}): Promise<string> {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': \`Bearer \${this.apiKey}\`,
      },
      body: JSON.stringify({
        model: options.model || 'gpt-4o-mini',
        messages: [
          ...(options.systemPrompt ? [{ role: 'system', content: options.systemPrompt }] : []),
          { role: 'user', content: prompt },
        ],
        max_tokens: options.maxTokens || 1000,
        temperature: options.temperature ?? 0.7,
      }),
    });

    if (!response.ok) {
      const error = await response.json().catch(() => ({}));
      throw new Error(error.error?.message || \`OpenAI error: \${response.status}\`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || '';
  }

  async *generateStream(prompt: string, options: GenerateOptions = {}): AsyncGenerator<string> {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': \`Bearer \${this.apiKey}\`,
      },
      body: JSON.stringify({
        model: options.model || 'gpt-4o-mini',
        messages: [
          ...(options.systemPrompt ? [{ role: 'system', content: options.systemPrompt }] : []),
          { role: 'user', content: prompt },
        ],
        max_tokens: options.maxTokens || 1000,
        temperature: options.temperature ?? 0.7,
        stream: true,
      }),
    });

    if (!response.ok) {
      throw new Error(\`OpenAI streaming error: \${response.status}\`);
    }

    const reader = response.body?.getReader();
    if (!reader) throw new Error('No response body');

    const decoder = new TextDecoder();
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const text = decoder.decode(value);
      const lines = text.split('\\n').filter(line => line.startsWith('data: '));

      for (const line of lines) {
        const data = line.slice(6);
        if (data === '[DONE]') continue;

        try {
          const parsed = JSON.parse(data);
          const content = parsed.choices[0]?.delta?.content;
          if (content) yield content;
        } catch {
          // Skip invalid JSON
        }
      }
    }
  }
}

// Anthropic Provider
class AnthropicProvider implements AIProvider {
  constructor(private apiKey: string) {}

  async generate(prompt: string, options: GenerateOptions = {}): Promise<string> {
    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': this.apiKey,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model: options.model || 'claude-3-haiku-20240307',
        max_tokens: options.maxTokens || 1000,
        system: options.systemPrompt,
        messages: [{ role: 'user', content: prompt }],
      }),
    });

    if (!response.ok) {
      const error = await response.json().catch(() => ({}));
      throw new Error(error.error?.message || \`Anthropic error: \${response.status}\`);
    }

    const data = await response.json();
    return data.content[0]?.text || '';
  }
}

// Ollama Provider (Local)
class OllamaProvider implements AIProvider {
  constructor(private baseUrl: string) {}

  async generate(prompt: string, options: GenerateOptions = {}): Promise<string> {
    const response = await fetch(\`\${this.baseUrl}/api/generate\`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: options.model || 'llama3.2',
        prompt: options.systemPrompt ? \`\${options.systemPrompt}\\n\\n\${prompt}\` : prompt,
        stream: false,
      }),
    });

    if (!response.ok) {
      throw new Error(\`Ollama error: \${response.statusText}\`);
    }

    const data = await response.json();
    return data.response;
  }

  async *generateStream(prompt: string, options: GenerateOptions = {}): AsyncGenerator<string> {
    const response = await fetch(\`\${this.baseUrl}/api/generate\`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: options.model || 'llama3.2',
        prompt: options.systemPrompt ? \`\${options.systemPrompt}\\n\\n\${prompt}\` : prompt,
        stream: true,
      }),
    });

    if (!response.ok) {
      throw new Error(\`Ollama streaming error: \${response.statusText}\`);
    }

    const reader = response.body?.getReader();
    if (!reader) throw new Error('No response body');

    const decoder = new TextDecoder();
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const text = decoder.decode(value);
      const lines = text.split('\\n').filter(Boolean);

      for (const line of lines) {
        try {
          const data = JSON.parse(line);
          if (data.response) yield data.response;
        } catch {
          // Skip invalid JSON
        }
      }
    }
  }
}

// Groq Provider (Fast inference)
class GroqProvider implements AIProvider {
  constructor(private apiKey: string) {}

  async generate(prompt: string, options: GenerateOptions = {}): Promise<string> {
    const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': \`Bearer \${this.apiKey}\`,
      },
      body: JSON.stringify({
        model: options.model || 'llama-3.1-70b-versatile',
        messages: [
          ...(options.systemPrompt ? [{ role: 'system', content: options.systemPrompt }] : []),
          { role: 'user', content: prompt },
        ],
        max_tokens: options.maxTokens || 1000,
        temperature: options.temperature ?? 0.7,
      }),
    });

    if (!response.ok) {
      const error = await response.json().catch(() => ({}));
      throw new Error(error.error?.message || \`Groq error: \${response.status}\`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || '';
  }
}

function createProvider(config: AIProviderConfig): AIProvider {
  switch (config.provider) {
    case 'openai':
      return new OpenAIProvider(config.apiKey!);
    case 'anthropic':
      return new AnthropicProvider(config.apiKey!);
    case 'ollama':
      return new OllamaProvider(config.baseUrl!);
    case 'groq':
      return new GroqProvider(config.apiKey!);
    default:
      throw new Error(\`Unknown provider: \${config.provider}\`);
  }
}

// Main AI Service with fallback support
export class AIService {
  private primary: AIProvider;
  private fallback: AIProvider | null;
  private primaryConfig: AIProviderConfig;

  constructor() {
    this.primaryConfig = getAIConfig();
    this.primary = createProvider(this.primaryConfig);

    const fallbackConfig = getFallbackConfig();
    this.fallback = fallbackConfig ? createProvider(fallbackConfig) : null;
  }

  async generate(prompt: string, options?: GenerateOptions): Promise<string> {
    try {
      return await this.primary.generate(prompt, {
        ...options,
        model: options?.model || this.primaryConfig.model,
      });
    } catch (error) {
      console.error('Primary AI provider failed:', error);

      if (this.fallback) {
        console.log('Falling back to local provider...');
        return await this.fallback.generate(prompt, options);
      }

      throw error;
    }
  }

  async *generateStream(prompt: string, options?: GenerateOptions): AsyncGenerator<string> {
    const provider = this.primary as AIProvider & { generateStream?: AIProvider['generateStream'] };

    if (!provider.generateStream) {
      // Fallback: return the full response at once
      const result = await this.generate(prompt, options);
      yield result;
      return;
    }

    try {
      yield* provider.generateStream(prompt, {
        ...options,
        model: options?.model || this.primaryConfig.model,
      });
    } catch (error) {
      console.error('Primary AI streaming failed:', error);

      if (this.fallback) {
        const fallbackProvider = this.fallback as AIProvider & { generateStream?: AIProvider['generateStream'] };
        if (fallbackProvider.generateStream) {
          yield* fallbackProvider.generateStream(prompt, options);
          return;
        }
      }

      throw error;
    }
  }
}

// Singleton instance
let aiService: AIService | null = null;

export function getAIService(): AIService {
  if (!aiService) {
    aiService = new AIService();
  }
  return aiService;
}

// Convenience function
export async function generate(prompt: string, options?: GenerateOptions): Promise<string> {
  return getAIService().generate(prompt, options);
}
`,
  };
}

/**
 * Generate the API route for AI generation
 */
export function generateAPIRouteFile(): ScaffoldFile {
  return {
    path: 'app/api/ai/generate/route.ts',
    content: `// AI Generation API Route
// Generated by AI Fleet Orchestrator

import { NextRequest, NextResponse } from 'next/server';
import { getAIService } from '@/lib/ai/service';

export async function POST(request: NextRequest) {
  try {
    const { prompt, options } = await request.json();

    if (!prompt) {
      return NextResponse.json(
        { error: 'Prompt is required' },
        { status: 400 }
      );
    }

    const ai = getAIService();
    const result = await ai.generate(prompt, options);

    return NextResponse.json({ result });
  } catch (error) {
    console.error('AI generation error:', error);

    const message = error instanceof Error ? error.message : 'AI service unavailable';

    // Check for specific error types
    if (message.includes('API key') || message.includes('401')) {
      return NextResponse.json(
        { error: 'AI service not configured. Please add your API key.' },
        { status: 503 }
      );
    }

    return NextResponse.json(
      { error: message },
      { status: 500 }
    );
  }
}
`,
  };
}

/**
 * Generate the streaming API route
 */
export function generateStreamAPIRouteFile(): ScaffoldFile {
  return {
    path: 'app/api/ai/stream/route.ts',
    content: `// AI Streaming API Route
// Generated by AI Fleet Orchestrator

import { NextRequest } from 'next/server';
import { getAIService } from '@/lib/ai/service';

export async function POST(request: NextRequest) {
  try {
    const { prompt, options } = await request.json();

    if (!prompt) {
      return new Response(
        JSON.stringify({ error: 'Prompt is required' }),
        { status: 400, headers: { 'Content-Type': 'application/json' } }
      );
    }

    const ai = getAIService();
    const encoder = new TextEncoder();

    const stream = new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of ai.generateStream(prompt, options)) {
            controller.enqueue(encoder.encode(\`data: \${JSON.stringify({ chunk })}\\n\\n\`));
          }
          controller.enqueue(encoder.encode('data: [DONE]\\n\\n'));
        } catch (error) {
          const message = error instanceof Error ? error.message : 'Stream failed';
          controller.enqueue(encoder.encode(\`data: \${JSON.stringify({ error: message })}\\n\\n\`));
        } finally {
          controller.close();
        }
      },
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });
  } catch (error) {
    console.error('AI streaming error:', error);
    return new Response(
      JSON.stringify({ error: 'Failed to start stream' }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
}
`,
  };
}

/**
 * Generate the React hook for AI
 */
export function generateHookFile(): ScaffoldFile {
  return {
    path: 'hooks/useAI.ts',
    content: `// AI React Hook
// Generated by AI Fleet Orchestrator

'use client';

import { useState, useCallback } from 'react';

export interface UseAIOptions {
  onError?: (error: Error) => void;
  onSuccess?: (result: string) => void;
}

export interface UseAIReturn {
  generate: (prompt: string) => Promise<string | null>;
  loading: boolean;
  error: string | null;
  result: string | null;
}

export function useAI(options: UseAIOptions = {}): UseAIReturn {
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [result, setResult] = useState<string | null>(null);

  const generate = useCallback(async (prompt: string): Promise<string | null> => {
    setLoading(true);
    setError(null);

    try {
      const response = await fetch('/api/ai/generate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || 'Generation failed');
      }

      setResult(data.result);
      options.onSuccess?.(data.result);
      return data.result;
    } catch (err) {
      const message = err instanceof Error ? err.message : 'Unknown error';
      setError(message);
      options.onError?.(err as Error);
      return null;
    } finally {
      setLoading(false);
    }
  }, [options]);

  return { generate, loading, error, result };
}

// Streaming version
export interface UseAIStreamReturn {
  generate: (prompt: string) => Promise<void>;
  loading: boolean;
  content: string;
  error: string | null;
  reset: () => void;
}

export function useAIStream(): UseAIStreamReturn {
  const [loading, setLoading] = useState(false);
  const [content, setContent] = useState('');
  const [error, setError] = useState<string | null>(null);

  const reset = useCallback(() => {
    setContent('');
    setError(null);
  }, []);

  const generate = useCallback(async (prompt: string): Promise<void> => {
    setLoading(true);
    setContent('');
    setError(null);

    try {
      const response = await fetch('/api/ai/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt }),
      });

      if (!response.ok) {
        const data = await response.json();
        throw new Error(data.error || 'Stream failed');
      }

      const reader = response.body?.getReader();
      if (!reader) throw new Error('No response body');

      const decoder = new TextDecoder();

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const text = decoder.decode(value);
        const lines = text.split('\\n').filter(line => line.startsWith('data: '));

        for (const line of lines) {
          const data = line.slice(6);
          if (data === '[DONE]') continue;

          try {
            const parsed = JSON.parse(data);
            if (parsed.chunk) {
              setContent(prev => prev + parsed.chunk);
            }
            if (parsed.error) {
              setError(parsed.error);
            }
          } catch {
            // Skip invalid JSON
          }
        }
      }
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Stream failed');
    } finally {
      setLoading(false);
    }
  }, []);

  return { generate, loading, content, error, reset };
}
`,
  };
}

/**
 * Generate the .env.example file
 */
export function generateEnvExampleFile(config: AIConfig): ScaffoldFile {
  return {
    path: '.env.example',
    content: `# AI Configuration
# Choose your AI provider and add the appropriate API key

# Default AI provider (openai | anthropic | ollama | groq)
AI_PROVIDER=${config.defaultProvider}

# OpenAI (https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=${config.providers.openai.defaultModel || 'gpt-4o-mini'}

# Anthropic (https://console.anthropic.com/)
ANTHROPIC_API_KEY=sk-ant-your-key-here
ANTHROPIC_MODEL=${config.providers.anthropic.defaultModel || 'claude-3-haiku-20240307'}

# Groq (https://console.groq.com/)
GROQ_API_KEY=gsk_your-key-here
GROQ_MODEL=${config.providers.groq.defaultModel || 'llama-3.1-70b-versatile'}

# Local LLM (Ollama - https://ollama.ai)
# Set ENABLE_LOCAL_FALLBACK=true to use Ollama when cloud providers fail
ENABLE_LOCAL_FALLBACK=false
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=${config.localLLM.ollama.defaultModel || 'llama3.2'}
`,
  };
}

/**
 * Generate the index export file
 */
export function generateIndexFile(): ScaffoldFile {
  return {
    path: 'lib/ai/index.ts',
    content: `// AI Module Exports
// Generated by AI Fleet Orchestrator

export * from './config';
export * from './service';
`,
  };
}

/**
 * Generate all AI scaffold files for a project
 */
export function generateAIScaffold(config: AIConfig): ScaffoldFile[] {
  const files: ScaffoldFile[] = [];

  if (config.builtAppSettings.includeServiceWrapper) {
    files.push(generateAIConfigFile(config));
    files.push(generateAIServiceFile());
    files.push(generateIndexFile());
    files.push(generateAPIRouteFile());
    files.push(generateStreamAPIRouteFile());
    files.push(generateHookFile());
  }

  if (config.builtAppSettings.includeEnvExample) {
    files.push(generateEnvExampleFile(config));
  }

  return files;
}
